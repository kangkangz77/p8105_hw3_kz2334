---
title: "p8105_hw3_kz2334"
author: "Kangkang Zhang"
date: "10/8/2018"
output: github_document
---

##Problem 1 


Load required packages.
```{r 1.1}
library(tidyverse)
devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)
```

---

Load the BRFSS data and do some data cleaning.

```{r 1.2}
data(brfss_smart2010)
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health", response %in% c("Excellent", "Very good", 
                                                  "Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, ordered = TRUE, levels = 
                          c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

---

Find out which states were observed at 7 locations in 2002.
```{r 1.3}
brfss_data %>% 
  distinct(locationdesc, locationabbr, year) %>% 
  count(locationabbr, year) %>% 
  filter(year == 2002, n == 7)
```

CT, FL NC were observed at 7 locations in 2002. It shows that the program had great attention to those three states in 2002.

---

Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.
```{r 1.4}
brfss_data %>% 
  group_by(locationabbr, year) %>% 
  summarise(n = n_distinct(locationdesc)) %>% 
ggplot(aes(y = n, x = year, color = locationabbr)) + 
  geom_line() + 
  labs(
    title = "spaghetti plot",
    x = "Year",
    y = "Number of Locations"
  ) +
  theme_bw() + 
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE
  ) 
  
```

We can see that the numbers of locations in majority of states fluctuated within [0, 10] from 2002 to 2010. There was a state with number of locations less than 10 in 2006, then rapidly grew up to larger than 40 in 2007. 

---

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r 1.5}
brfss_data %>%
  filter(year %in% c(2002, 2006, 2010), locationabbr == "NY") %>% 
  group_by(year, response) %>% 
  summarise(mean = mean(data_value),
            standard_deviation = sd(data_value)) %>% 
  filter(response == "Excellent") %>% 
knitr::kable(digits = 2)
```

The mean of proportion of "Excellent" Response in locations in NY varied a little among those years. The standard deviation of "Excellent" Response in locations in NY were almost the same among those years.

---

Make a five-panel plot that shows the average proportion in each response category for each year and each state.

```{r 1.6}
brfss_data %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(mean_proportion = mean(data_value, na.rm =TRUE)) %>% 
ggplot(aes(y = mean_proportion, x = year, color = locationabbr)) + 
  geom_line(show.legend = FALSE) + 
  labs(
    title = "panel plot category by response",
    x = "Year",
    y = "Proportion of Response"
  ) +
  facet_grid( ~ response) + 
  theme_bw() + 
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE)
```

We can see from the plot that globally, "Very Good" has the largest proportion among those response. The distribution of proportion of "Good" is very close to the distribution of "Very Good". The proportion of "Excellent" has median value among them. The proportion of "Poor" is the least among five repsonses.

##Problem 2

Load the instacart data and do some data cleaning.

```{r 2.1}
data(instacart)
instacart_data = instacart %>% 
  janitor::clean_names() 

instacart_data
```

Instacart dataset tells imformation about transactions of online grocery shopping. 

*   The size of the dataset is (`r dim(instacart_data)`). 
*   It is a dataframe, containing integer and chracter variables. 
*   There are `r count(distinct(instacart_data, order_id))` unique orders.
*   `reordered` equals 1 if this prodcut has been ordered by this user in the past, 0 otherwise. `r round(mean(instacart_data$reordered)*100, 2)`% of products were not bought for the first time.
*   `order_hour_of_day` recorded the hour of the day on which the order was placed.
*   The maximum times of orders for one person is `r max(instacart_data$order_number)`.
*   `order_dow` represents the day of the week on which the order was placed. In which, I assume that `0` represents Sunday, `1 - 6` represents Monday to Friday.

---

Find out how many aisles are there, and which aisles are the most items ordered from.

```{r 2.2}
instacart_data %>% 
  group_by(aisle_id, aisle) %>% 
  summarise( n = n_distinct(product_id)) %>% 
  arrange(desc(n))
```
There are 134 aisles, and most items ordered, whose number is 943, are from  candy chocolate. Top 3 items, lucluding chocolate, yogurt and ice cream, are all desserts. 

---

Make a plot that shows the number of items ordered in each aisle. 
```{r 2.3}
instacart_data %>% 
  distinct(product_id, aisle_id, aisle) %>% 
  ggplot(aes(x = aisle_id)) +
  geom_histogram( bins = 134, fill = "black", color = "white") +
  scale_x_continuous(breaks = seq(0, 134, 5)) +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle ID",
    y = "Number of items"
  ) +
  theme_bw()
```

The number of items ordered in each aisle varied a lot. The maximum almost reachs 1000, while the minimum is close to zero.

---

Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r 2.4, warning = FALSE}
instacart_data %>% 
  group_by(aisle, product_name) %>% 
  summarise( n = n()) %>% 
  filter(min_rank(desc(n)) < 2, aisle %in% c("baking ingredients", 
                                        "dog food care", "packaged vegetables fruits")) %>% 
  knitr::kable(digits = 0)
```

In “baking ingredients”, Light Brown Sugar orderer by 499 times is the most popular product. In “packaged vegetables fruits”, Organic Baby Spinach is the most popular, ordered by 9784 times. The most popular product in “dog food care” was only ordered by 30 times.

---

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r 2.5}
instacart_data %>%
  group_by(order_dow, product_name) %>% 
  filter(product_name %in% c(
    "Pink Lady Apples", "Coffee Ice Cream")) %>% 
  summarise(mean_hour = mean(order_hour_of_day, rm.na = TRUE)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable(digits = 1)
```

Globally, Coffee Ice Cream was ordered later than Pink Lady Apples every weekday except for Friday. The mean hour of the day Coffee Ice Cream was ordered is later than 12am every weekday, while the mean hour of four weekdays Pink Lady Apples was ordered earlier than 12am.

---

##Problem 3

Load the NY NOAA data and do some data cleaning.
```{r 3.1}
data(ny_noaa)
ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  mutate(tmax = as.integer(tmax), tmin = as.integer((tmin)))

ny_noaa
```

Calculate number of missing values in each column.
```{r 3.2}
ny_noaa %>% 
  summarise_all(funs(sum(is.na(.)))) %>% 
  knitr::kable()
```

NY NOAA dataset tells imformation about snow weather in NYC. 

*   The size of the dataset is (`r dim(ny_noaa)`). 
*   It contains integer, date and chracter variables. 
*   tmax and tmin have the most missing values, extent to around 1 million.This is because about one half of the stations report precipitation only.
*   The maximum snowfall in one day is `r max(ny_noaa$snow, na.rm = TRUE)` mm.
*   The mean of precipitation is `r round(mean(ny_noaa$prcp, na.rm = TRUE)/10, 2)` mm.
*   The mean of max temprature is `r round(mean(ny_noaa$tmax, na.rm = TRUE)/10, 2)` ºC.

---

Create separate variables for year, month, and day. Ensure variables are given in reasonable units.
```{r 3.3}
ny_noaa = 
  ny_noaa %>% 
  separate(date, c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp/10, tmax = tmax/10, tmin = tmin/10, 
         year = as.integer(year), month = as.integer(month),
              day = as.integer(day))

ny_noaa
```

Find out the most commonly observed values for snowfall. Top 5 values are shown below.
```{r 3.4}
ny_noaa %>% 
  group_by(snow) %>% 
  summarise( n_obs = n()) %>% 
  arrange(desc(n_obs)) %>% 
  head(5) %>% 
  knitr::kable()
```

The most commonly observed values for snowfall is 0, 25, 13 and 51. Majority of time there were no snowfall in New York states.

---

Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r 3.5}
ny_noaa %>% 
  filter(month %in% c(01, 07), !is.na(tmax)) %>% 
  group_by(id, year, month) %>% 
  summarise(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = id)) +
  geom_line(show.legend = FALSE) + 
  labs(
    title = "Maximum temperature in January and July", 
    x = "year",
    y = "Maximum temperature (ºC)"
  ) + 
  facet_grid( ~ month) + 
  viridis::scale_color_viridis(discrete = TRUE)
```

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option) (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r 3.6}
devtools::install_github("thomasp85/patchwork")
library(patchwork)

plot1 = ny_noaa %>% 
  filter(!is.na(tmax) & !is.na(tmin)) %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex(bins = 50) +
  labs(
    title = "Maximum temperature vs Minimum temperature",
    x = "Maximum temperature (ºC)",
    y = "Minimum temperature (ºC)"
  ) 

plot2 = ny_noaa %>% 
  filter(snow > 0 & snow < 100) %>% 
  mutate(year = as.factor(year)) %>% 
  ggplot() +
  geom_boxplot(aes(x = year, y = snow), alpha = .4) + 
  labs(
    title = "Distribution of snowfall values by year",
    x = "ear",
    y = "Snowfall(mm)"
  ) +
  viridis::scale_fill_viridis(discrete = TRUE) + 
  theme_bw() 

plot1 / plot2
```

