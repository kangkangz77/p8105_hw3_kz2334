p8105\_hw3\_kz2334
================
Kangkang Zhang
10/8/2018

Problem 1
---------

Load required packages.

``` r
library(tidyverse)
```

    ## ── Attaching packages ──────────────────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.0.0     ✔ purrr   0.2.5
    ## ✔ tibble  1.4.2     ✔ dplyr   0.7.6
    ## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
    ## ✔ readr   1.1.1     ✔ forcats 0.3.0

    ## ── Conflicts ─────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
devtools::install_github("p8105/p8105.datasets")
```

    ## Skipping install of 'p8105.datasets' from a github remote, the SHA1 (21f5ad1c) has not changed since last install.
    ##   Use `force = TRUE` to force installation

``` r
library(p8105.datasets)
```

------------------------------------------------------------------------

Load the BRFSS data and do some data cleaning.

``` r
data(brfss_smart2010)
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health", response %in% c("Excellent", "Very good", 
                                                  "Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, ordered = TRUE, levels = 
                          c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

------------------------------------------------------------------------

Find out which states were observed at 7 locations in 2002.

``` r
brfss_data %>% 
  distinct(locationdesc, locationabbr, year) %>% 
  count(locationabbr, year) %>% 
  filter(year == 2002, n == 7)
```

    ## # A tibble: 3 x 3
    ##   locationabbr  year     n
    ##   <chr>        <int> <int>
    ## 1 CT            2002     7
    ## 2 FL            2002     7
    ## 3 NC            2002     7

CT, FL NC were observed at 7 locations in 2002. It shows that the program had great attention to those three states in 2002.

------------------------------------------------------------------------

Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

``` r
brfss_data %>% 
  group_by(locationabbr, year) %>% 
  summarise(n = n_distinct(locationdesc)) %>% 
ggplot(aes(y = n, x = year, color = locationabbr)) + 
  geom_line() + 
  labs(
    title = "spaghetti plot",
    x = "Year",
    y = "Number of Locations"
  ) +
  theme_bw() + 
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE
  ) 
```

![](p8105_hw3_kz2334_files/figure-markdown_github/1.4-1.png)

We can see that the numbers of locations in majority of states fluctuated within \[0, 10\] from 2002 to 2010. There was a state with number of locations less than 10 in 2006, then rapidly grew up to larger than 40 in 2007.

------------------------------------------------------------------------

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
brfss_data %>%
  filter(year %in% c(2002, 2006, 2010), locationabbr == "NY") %>% 
  group_by(year, response) %>% 
  summarise(mean = mean(data_value),
            standard_deviation = sd(data_value)) %>% 
  filter(response == "Excellent") %>% 
knitr::kable(digits = 2)
```

|  year| response  |   mean|  standard\_deviation|
|-----:|:----------|------:|--------------------:|
|  2002| Excellent |  24.04|                 4.49|
|  2006| Excellent |  22.53|                 4.00|
|  2010| Excellent |  22.70|                 3.57|

The mean of proportion of "Excellent" Response in locations in NY varied a little among those years. The standard deviation of "Excellent" Response in locations in NY were almost the same among those years.

------------------------------------------------------------------------

Make a five-panel plot that shows the average proportion in each response category for each year and each state.

``` r
brfss_data %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(mean_proportion = mean(data_value, na.rm =TRUE)) %>% 
ggplot(aes(y = mean_proportion, x = year, color = locationabbr)) + 
  geom_line(show.legend = FALSE) + 
  labs(
    title = "panel plot category by response",
    x = "Year",
    y = "Proportion of Response"
  ) +
  facet_grid( ~ response) + 
  theme_bw() + 
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE)
```

![](p8105_hw3_kz2334_files/figure-markdown_github/1.6-1.png)

We can see from the plot that globally, "Very Good" has the largest proportion among those response. The distribution of proportion of "Good" is very close to the distribution of "Very Good". The proportion of "Excellent" has median value among them. The proportion of "Poor" is the least among five repsonses.

Problem 2
---------

Load the instacart data and do some data cleaning.

``` r
data(instacart)
instacart_data = instacart %>% 
  janitor::clean_names() 

instacart_data
```

    ## # A tibble: 1,384,617 x 15
    ##    order_id product_id add_to_cart_ord… reordered user_id eval_set
    ##       <int>      <int>            <int>     <int>   <int> <chr>   
    ##  1        1      49302                1         1  112108 train   
    ##  2        1      11109                2         1  112108 train   
    ##  3        1      10246                3         0  112108 train   
    ##  4        1      49683                4         0  112108 train   
    ##  5        1      43633                5         1  112108 train   
    ##  6        1      13176                6         0  112108 train   
    ##  7        1      47209                7         0  112108 train   
    ##  8        1      22035                8         1  112108 train   
    ##  9       36      39612                1         0   79431 train   
    ## 10       36      19660                2         1   79431 train   
    ## # ... with 1,384,607 more rows, and 9 more variables: order_number <int>,
    ## #   order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

Instacart dataset tells imformation about transactions of online grocery shopping.

-   The size of the dataset is (1384617, 15).
-   It is a dataframe, containing integer and chracter variables.
-   There are 131209 unique orders.
-   `reordered` equals 1 if this prodcut has been ordered by this user in the past, 0 otherwise. 59.86% of products were not bought for the first time.
-   `order_hour_of_day` recorded the hour of the day on which the order was placed.
-   The maximum times of orders for one person is 100.
-   `order_dow` represents the day of the week on which the order was placed. In which, I assume that `0` represents Sunday, `1 - 6` represents Monday to Friday.

------------------------------------------------------------------------

Find out how many aisles are there, and which aisles are the most items ordered from.

``` r
instacart_data %>% 
  group_by(aisle_id, aisle) %>% 
  summarise( n = n_distinct(product_id)) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 x 3
    ## # Groups:   aisle_id [134]
    ##    aisle_id aisle                   n
    ##       <int> <chr>               <int>
    ##  1       45 candy chocolate       943
    ##  2      120 yogurt                911
    ##  3      100 missing               905
    ##  4       37 ice cream ice         901
    ##  5      107 chips pretzels        844
    ##  6       21 packaged cheese       791
    ##  7       38 frozen meals          757
    ##  8        3 energy granola bars   690
    ##  9       94 tea                   674
    ## 10       61 cookies cakes         654
    ## # ... with 124 more rows

There are 134 aisles, and most items ordered, whose number is 943, are from candy chocolate. Top 3 items, lucluding chocolate, yogurt and ice cream, are all desserts.

------------------------------------------------------------------------

Make a plot that shows the number of items ordered in each aisle.

``` r
instacart_data %>% 
  distinct(product_id, aisle_id, aisle) %>% 
  ggplot(aes(x = aisle_id)) +
  geom_histogram( bins = 134, fill = "black", color = "white") +
  scale_x_continuous(breaks = seq(0, 134, 5)) +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle ID",
    y = "Number of items"
  ) +
  theme_bw()
```

![](p8105_hw3_kz2334_files/figure-markdown_github/2.3-1.png)

The number of items ordered in each aisle varied a lot. The maximum almost reachs 1000, while the minimum is close to zero.

------------------------------------------------------------------------

Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

``` r
instacart_data %>% 
  group_by(aisle, product_name) %>% 
  summarise( n = n()) %>% 
  filter(min_rank(desc(n)) < 2, aisle %in% c("baking ingredients", 
                                        "dog food care", "packaged vegetables fruits")) %>% 
  knitr::kable(digits = 0)
```

| aisle                      | product\_name                                 |     n|
|:---------------------------|:----------------------------------------------|-----:|
| baking ingredients         | Light Brown Sugar                             |   499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |    30|
| packaged vegetables fruits | Organic Baby Spinach                          |  9784|

In “baking ingredients”, Light Brown Sugar orderer by 499 times is the most popular product. In “packaged vegetables fruits”, Organic Baby Spinach is the most popular, ordered by 9784 times. The most popular product in “dog food care” was only ordered by 30 times.

------------------------------------------------------------------------

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

``` r
instacart_data %>%
  group_by(order_dow, product_name) %>% 
  filter(product_name %in% c(
    "Pink Lady Apples", "Coffee Ice Cream")) %>% 
  summarise(mean_hour = mean(order_hour_of_day, rm.na = TRUE)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable(digits = 1)
```

| product\_name    |     0|     1|     2|     3|     4|     5|     6|
|:-----------------|-----:|-----:|-----:|-----:|-----:|-----:|-----:|
| Coffee Ice Cream |  13.8|  14.3|  15.4|  15.3|  15.2|  12.3|  13.8|
| Pink Lady Apples |  13.4|  11.4|  11.7|  14.2|  11.6|  12.8|  11.9|

Globally, Coffee Ice Cream was ordered later than Pink Lady Apples every weekday except for Friday. The mean hour of the day Coffee Ice Cream was ordered is later than 12am every weekday, while the mean hour of four weekdays Pink Lady Apples was ordered earlier than 12am.

------------------------------------------------------------------------

Problem 3
---------

Load the NY NOAA data and do some data cleaning.

``` r
data(ny_noaa)
ny_noaa = ny_noaa %>% 
  janitor::clean_names() %>% 
  mutate(tmax = as.integer(tmax), tmin = as.integer((tmin)))

ny_noaa
```

    ## # A tibble: 2,595,176 x 7
    ##    id          date        prcp  snow  snwd  tmax  tmin
    ##    <chr>       <date>     <int> <int> <int> <int> <int>
    ##  1 US1NYAB0001 2007-11-01    NA    NA    NA    NA    NA
    ##  2 US1NYAB0001 2007-11-02    NA    NA    NA    NA    NA
    ##  3 US1NYAB0001 2007-11-03    NA    NA    NA    NA    NA
    ##  4 US1NYAB0001 2007-11-04    NA    NA    NA    NA    NA
    ##  5 US1NYAB0001 2007-11-05    NA    NA    NA    NA    NA
    ##  6 US1NYAB0001 2007-11-06    NA    NA    NA    NA    NA
    ##  7 US1NYAB0001 2007-11-07    NA    NA    NA    NA    NA
    ##  8 US1NYAB0001 2007-11-08    NA    NA    NA    NA    NA
    ##  9 US1NYAB0001 2007-11-09    NA    NA    NA    NA    NA
    ## 10 US1NYAB0001 2007-11-10    NA    NA    NA    NA    NA
    ## # ... with 2,595,166 more rows

Calculate number of missing values in each column.

``` r
ny_noaa %>% 
  summarise_all(funs(sum(is.na(.)))) %>% 
  knitr::kable()
```

|   id|  date|    prcp|    snow|    snwd|     tmax|     tmin|
|----:|-----:|-------:|-------:|-------:|--------:|--------:|
|    0|     0|  145838|  381221|  591786|  1134358|  1134420|

NY NOAA dataset tells imformation about snow weather in NYC.

-   The size of the dataset is (2595176, 7).
-   It contains integer, date and chracter variables.
-   tmax and tmin have the most missing values, extent to around 1 million.This is because about one half of the stations report precipitation only.
-   The maximum snowfall in one day is 10160 mm.
-   The mean of precipitation is 2.98 mm.
-   The mean of max temprature is 13.98 ºC.

------------------------------------------------------------------------

Create separate variables for year, month, and day. Ensure variables are given in reasonable units.

``` r
ny_noaa = 
  ny_noaa %>% 
  separate(date, c("year", "month", "day"), sep = "-") %>% 
  mutate(prcp = prcp/10, tmax = tmax/10, tmin = tmin/10, 
         year = as.integer(year), month = as.integer(month),
              day = as.integer(day))

ny_noaa
```

    ## # A tibble: 2,595,176 x 9
    ##    id           year month   day  prcp  snow  snwd  tmax  tmin
    ##    <chr>       <int> <int> <int> <dbl> <int> <int> <dbl> <dbl>
    ##  1 US1NYAB0001  2007    11     1    NA    NA    NA    NA    NA
    ##  2 US1NYAB0001  2007    11     2    NA    NA    NA    NA    NA
    ##  3 US1NYAB0001  2007    11     3    NA    NA    NA    NA    NA
    ##  4 US1NYAB0001  2007    11     4    NA    NA    NA    NA    NA
    ##  5 US1NYAB0001  2007    11     5    NA    NA    NA    NA    NA
    ##  6 US1NYAB0001  2007    11     6    NA    NA    NA    NA    NA
    ##  7 US1NYAB0001  2007    11     7    NA    NA    NA    NA    NA
    ##  8 US1NYAB0001  2007    11     8    NA    NA    NA    NA    NA
    ##  9 US1NYAB0001  2007    11     9    NA    NA    NA    NA    NA
    ## 10 US1NYAB0001  2007    11    10    NA    NA    NA    NA    NA
    ## # ... with 2,595,166 more rows

Find out the most commonly observed values for snowfall. Top 5 values are shown below.

``` r
ny_noaa %>% 
  group_by(snow) %>% 
  summarise( n_obs = n()) %>% 
  arrange(desc(n_obs)) %>% 
  head(5) %>% 
  knitr::kable()
```

|  snow|   n\_obs|
|-----:|--------:|
|     0|  2008508|
|    NA|   381221|
|    25|    31022|
|    13|    23095|
|    51|    18274|

The most commonly observed values for snowfall is 0, 25, 13 and 51. Majority of time there were no snowfall in New York states.

------------------------------------------------------------------------

Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

``` r
ny_noaa %>% 
  filter(month %in% c(01, 07), !is.na(tmax)) %>% 
  group_by(id, year, month) %>% 
  summarise(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_tmax, color = id)) +
  geom_point(show.legend = FALSE) + 
  labs(
    title = "Maximum temperature in January and July", 
    x = "year",
    y = "Maximum temperature (ºC)"
  ) + 
  facet_grid( ~ month) + 
  viridis::scale_color_viridis(discrete = TRUE)
```

![](p8105_hw3_kz2334_files/figure-markdown_github/3.5-1.png)

We can see that mean maximum tempature in July is globally higher than it in January. We can find some spots is out the area of majority of spots. For example, in 1981 January, a purple spot is much lower than other spots.
