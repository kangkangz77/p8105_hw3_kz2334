p8105\_hw3\_kz2334
================
Kangkang Zhang
10/8/2018

Problem 1
---------

Load required packages.

``` r
library(tidyverse)
```

    ## ── Attaching packages ──────────────────────────────────────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.0.0     ✔ purrr   0.2.5
    ## ✔ tibble  1.4.2     ✔ dplyr   0.7.6
    ## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
    ## ✔ readr   1.1.1     ✔ forcats 0.3.0

    ## ── Conflicts ─────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
devtools::install_github("p8105/p8105.datasets")
```

    ## Skipping install of 'p8105.datasets' from a github remote, the SHA1 (21f5ad1c) has not changed since last install.
    ##   Use `force = TRUE` to force installation

``` r
library(p8105.datasets)
```

------------------------------------------------------------------------

Load the BRFSS data and do some data cleaning.

``` r
data(brfss_smart2010)
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health", response %in% c("Excellent", "Very good", 
                                                  "Good", "Fair", "Poor")) %>% 
  mutate(response = factor(response, ordered = TRUE, levels = 
                          c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

------------------------------------------------------------------------

Find out which states were observed at 7 locations in 2002.

``` r
brfss_data %>% 
  distinct(locationdesc, locationabbr, year) %>% 
  count(locationabbr, year) %>% 
  filter(year == 2002, n == 7)
```

    ## # A tibble: 3 x 3
    ##   locationabbr  year     n
    ##   <chr>        <int> <int>
    ## 1 CT            2002     7
    ## 2 FL            2002     7
    ## 3 NC            2002     7

CT, FL NC were observed at 7 locations in 2002. It shows that the program had great attention to those three states in 2002.

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
brfss_data %>%
  filter(year %in% c(2002, 2006, 2010), locationabbr == "NY") %>% 
  group_by(year, response) %>% 
  summarise(mean = mean(data_value),
            standard_deviation = sd(data_value)) %>% 
  filter(response == "Excellent") %>% 
knitr::kable(digits = 2)
```

|  year| response  |   mean|  standard\_deviation|
|-----:|:----------|------:|--------------------:|
|  2002| Excellent |  24.04|                 4.49|
|  2006| Excellent |  22.53|                 4.00|
|  2010| Excellent |  22.70|                 3.57|

The mean of proportion of "Excellent" Response in locations in NY varied a little among those years. The standard deviation of "Excellent" Response in locations in NY were almost the same among those years.

------------------------------------------------------------------------

Make a five-panel plot that shows the average proportion in each response category for each year and each state.

``` r
brfss_data %>% 
  group_by(year, locationabbr, response) %>% 
  summarise(mean_proportion = mean(data_value, na.rm =TRUE)) %>% 
ggplot(aes(y = mean_proportion, x = year, color = locationabbr)) + 
  geom_line(show.legend=F) + 
  labs(
    title = "panel plot category by response",
    x = "Year",
    y = "Proportion of Response"
  ) +
  facet_grid( ~ response) + 
  theme_bw() + 
  viridis::scale_color_viridis(
    name = "State", 
    discrete = TRUE)
```

![](p8105_hw3_kz2334_files/figure-markdown_github/1.6-1.png)

We can see from the plot that globally, "Very Good" has the largest proportion among those response. The distribution of proportion of "Good" is very close to the distribution of "Very Good". The proportion of "Excellent" has median value among them. The proportion of "Poor" is the least among them.

Problem 2
---------

Load the instacart data and do some data cleaning.

``` r
data(instacart)
instacart_data = instacart %>% 
  janitor::clean_names() 

instacart_data
```

    ## # A tibble: 1,384,617 x 15
    ##    order_id product_id add_to_cart_ord… reordered user_id eval_set
    ##       <int>      <int>            <int>     <int>   <int> <chr>   
    ##  1        1      49302                1         1  112108 train   
    ##  2        1      11109                2         1  112108 train   
    ##  3        1      10246                3         0  112108 train   
    ##  4        1      49683                4         0  112108 train   
    ##  5        1      43633                5         1  112108 train   
    ##  6        1      13176                6         0  112108 train   
    ##  7        1      47209                7         0  112108 train   
    ##  8        1      22035                8         1  112108 train   
    ##  9       36      39612                1         0   79431 train   
    ## 10       36      19660                2         1   79431 train   
    ## # ... with 1,384,607 more rows, and 9 more variables: order_number <int>,
    ## #   order_dow <int>, order_hour_of_day <int>,
    ## #   days_since_prior_order <int>, product_name <chr>, aisle_id <int>,
    ## #   department_id <int>, aisle <chr>, department <chr>

Instacart dataset tells imformation about transactions of online grocery shopping.

-   The size of the dataset is (1384617, 15).
-   It contains integer and chracter variables.
-   There are 131209 unique orders.
-   59.86% of products were not bought for the first time.
-   The maximum times of orders for one person is 100.
